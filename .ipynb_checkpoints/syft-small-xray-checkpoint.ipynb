{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 20\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 10\n",
    "        self.save_model = False\n",
    "args = Arguments()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {}\n",
    "pt = 0\n",
    "for i in pd.read_csv('./csv/train.csv')['label'].unique():\n",
    "    classes[i] = pt\n",
    "    pt+=1\n",
    "def get_onehot(label):\n",
    "    return classes[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        img_name = os.path.join(self.root_dir,label,\n",
    "                                str(self.df.iloc[idx]['image']))\n",
    "        image = Image.open(img_name)\n",
    "        image = PIL.ImageOps.grayscale(image)\n",
    "        onehot = np.array(get_onehot(label))\n",
    "#         landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image,onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The following options are not supported: num_workers: 1, pin_memory: True\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./csv/train.csv')\n",
    "df_train = df_train.sample(frac=1)\n",
    "traindataset = XDataset(df_train,root_dir='./x-ray/train/',\n",
    "                                        transform=\n",
    "                                 transforms.Compose([transforms.Resize((28,28)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize((0.5,), (0.5,))]))\n",
    "\n",
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    traindataset.federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "df_test = pd.read_csv('./csv/test.csv')\n",
    "df_test = df_test.sample(frac=1)\n",
    "testdataset = XDataset(df_test,root_dir='./x-ray/test/',\n",
    "                                        transform=\n",
    "                                transforms.Compose([transforms.Resize((28,28)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize((0.5,), (0.5,))]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testdataset,\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5248 (0%)]\tLoss: 2.312666\n",
      "Train Epoch: 1 [640/5248 (12%)]\tLoss: 2.171727\n",
      "Train Epoch: 1 [1280/5248 (24%)]\tLoss: 2.014336\n",
      "Train Epoch: 1 [1920/5248 (37%)]\tLoss: 1.674796\n",
      "Train Epoch: 1 [2560/5248 (49%)]\tLoss: 1.056160\n",
      "Train Epoch: 1 [3200/5248 (61%)]\tLoss: 0.823476\n",
      "Train Epoch: 1 [3840/5248 (73%)]\tLoss: 0.727140\n",
      "Train Epoch: 1 [4480/5248 (85%)]\tLoss: 0.694994\n",
      "Train Epoch: 1 [5120/5248 (98%)]\tLoss: 0.658833\n",
      "\n",
      "Test set: Average loss: 0.7127, Accuracy: 390/624 (62%)\n",
      "\n",
      "Train Epoch: 2 [0/5248 (0%)]\tLoss: 0.568777\n",
      "Train Epoch: 2 [640/5248 (12%)]\tLoss: 0.596642\n",
      "Train Epoch: 2 [1280/5248 (24%)]\tLoss: 0.644283\n",
      "Train Epoch: 2 [1920/5248 (37%)]\tLoss: 0.479247\n",
      "Train Epoch: 2 [2560/5248 (49%)]\tLoss: 0.577734\n",
      "Train Epoch: 2 [3200/5248 (61%)]\tLoss: 0.517618\n",
      "Train Epoch: 2 [3840/5248 (73%)]\tLoss: 0.616605\n",
      "Train Epoch: 2 [4480/5248 (85%)]\tLoss: 0.559338\n",
      "Train Epoch: 2 [5120/5248 (98%)]\tLoss: 0.578986\n",
      "\n",
      "Test set: Average loss: 0.6914, Accuracy: 390/624 (62%)\n",
      "\n",
      "Train Epoch: 3 [0/5248 (0%)]\tLoss: 0.580150\n",
      "Train Epoch: 3 [640/5248 (12%)]\tLoss: 0.549250\n",
      "Train Epoch: 3 [1280/5248 (24%)]\tLoss: 0.487016\n",
      "Train Epoch: 3 [1920/5248 (37%)]\tLoss: 0.530268\n",
      "Train Epoch: 3 [2560/5248 (49%)]\tLoss: 0.517495\n",
      "Train Epoch: 3 [3200/5248 (61%)]\tLoss: 0.518400\n",
      "Train Epoch: 3 [3840/5248 (73%)]\tLoss: 0.511017\n",
      "Train Epoch: 3 [4480/5248 (85%)]\tLoss: 0.600168\n",
      "Train Epoch: 3 [5120/5248 (98%)]\tLoss: 0.439113\n",
      "\n",
      "Test set: Average loss: 0.6083, Accuracy: 390/624 (62%)\n",
      "\n",
      "Train Epoch: 4 [0/5248 (0%)]\tLoss: 0.477769\n",
      "Train Epoch: 4 [640/5248 (12%)]\tLoss: 0.473732\n",
      "Train Epoch: 4 [1280/5248 (24%)]\tLoss: 0.481320\n",
      "Train Epoch: 4 [1920/5248 (37%)]\tLoss: 0.484611\n",
      "Train Epoch: 4 [2560/5248 (49%)]\tLoss: 0.497871\n",
      "Train Epoch: 4 [3200/5248 (61%)]\tLoss: 0.466496\n",
      "Train Epoch: 4 [3840/5248 (73%)]\tLoss: 0.490450\n",
      "Train Epoch: 4 [4480/5248 (85%)]\tLoss: 0.453779\n",
      "Train Epoch: 4 [5120/5248 (98%)]\tLoss: 0.474819\n",
      "\n",
      "Test set: Average loss: 0.5198, Accuracy: 443/624 (71%)\n",
      "\n",
      "Train Epoch: 5 [0/5248 (0%)]\tLoss: 0.470197\n",
      "Train Epoch: 5 [640/5248 (12%)]\tLoss: 0.340661\n",
      "Train Epoch: 5 [1280/5248 (24%)]\tLoss: 0.405436\n",
      "Train Epoch: 5 [1920/5248 (37%)]\tLoss: 0.401770\n",
      "Train Epoch: 5 [2560/5248 (49%)]\tLoss: 0.309121\n",
      "Train Epoch: 5 [3200/5248 (61%)]\tLoss: 0.448197\n",
      "Train Epoch: 5 [3840/5248 (73%)]\tLoss: 0.362137\n",
      "Train Epoch: 5 [4480/5248 (85%)]\tLoss: 0.207918\n",
      "Train Epoch: 5 [5120/5248 (98%)]\tLoss: 0.504401\n",
      "\n",
      "Test set: Average loss: 0.5130, Accuracy: 454/624 (73%)\n",
      "\n",
      "Train Epoch: 6 [0/5248 (0%)]\tLoss: 0.271603\n",
      "Train Epoch: 6 [640/5248 (12%)]\tLoss: 0.390063\n",
      "Train Epoch: 6 [1280/5248 (24%)]\tLoss: 0.275383\n",
      "Train Epoch: 6 [1920/5248 (37%)]\tLoss: 0.345440\n",
      "Train Epoch: 6 [2560/5248 (49%)]\tLoss: 0.247198\n",
      "Train Epoch: 6 [3200/5248 (61%)]\tLoss: 0.353795\n",
      "Train Epoch: 6 [3840/5248 (73%)]\tLoss: 0.322413\n",
      "Train Epoch: 6 [4480/5248 (85%)]\tLoss: 0.245256\n",
      "Train Epoch: 6 [5120/5248 (98%)]\tLoss: 0.271279\n",
      "\n",
      "Test set: Average loss: 0.4209, Accuracy: 512/624 (82%)\n",
      "\n",
      "Train Epoch: 7 [0/5248 (0%)]\tLoss: 0.303839\n",
      "Train Epoch: 7 [640/5248 (12%)]\tLoss: 0.337644\n",
      "Train Epoch: 7 [1280/5248 (24%)]\tLoss: 0.294625\n",
      "Train Epoch: 7 [1920/5248 (37%)]\tLoss: 0.370644\n",
      "Train Epoch: 7 [2560/5248 (49%)]\tLoss: 0.268026\n",
      "Train Epoch: 7 [3200/5248 (61%)]\tLoss: 0.235886\n",
      "Train Epoch: 7 [3840/5248 (73%)]\tLoss: 0.191973\n",
      "Train Epoch: 7 [4480/5248 (85%)]\tLoss: 0.197276\n",
      "Train Epoch: 7 [5120/5248 (98%)]\tLoss: 0.316431\n",
      "\n",
      "Test set: Average loss: 0.4038, Accuracy: 520/624 (83%)\n",
      "\n",
      "Train Epoch: 8 [0/5248 (0%)]\tLoss: 0.209541\n",
      "Train Epoch: 8 [640/5248 (12%)]\tLoss: 0.232209\n",
      "Train Epoch: 8 [1280/5248 (24%)]\tLoss: 0.222858\n",
      "Train Epoch: 8 [1920/5248 (37%)]\tLoss: 0.319162\n",
      "Train Epoch: 8 [2560/5248 (49%)]\tLoss: 0.206889\n",
      "Train Epoch: 8 [3200/5248 (61%)]\tLoss: 0.247641\n",
      "Train Epoch: 8 [3840/5248 (73%)]\tLoss: 0.151976\n",
      "Train Epoch: 8 [4480/5248 (85%)]\tLoss: 0.189719\n",
      "Train Epoch: 8 [5120/5248 (98%)]\tLoss: 0.186026\n",
      "\n",
      "Test set: Average loss: 0.5547, Accuracy: 474/624 (76%)\n",
      "\n",
      "Train Epoch: 9 [0/5248 (0%)]\tLoss: 0.176835\n",
      "Train Epoch: 9 [640/5248 (12%)]\tLoss: 0.230703\n",
      "Train Epoch: 9 [1280/5248 (24%)]\tLoss: 0.097437\n",
      "Train Epoch: 9 [1920/5248 (37%)]\tLoss: 0.162252\n",
      "Train Epoch: 9 [2560/5248 (49%)]\tLoss: 0.271542\n",
      "Train Epoch: 9 [3200/5248 (61%)]\tLoss: 0.164730\n",
      "Train Epoch: 9 [3840/5248 (73%)]\tLoss: 0.239392\n",
      "Train Epoch: 9 [4480/5248 (85%)]\tLoss: 0.266558\n",
      "Train Epoch: 9 [5120/5248 (98%)]\tLoss: 0.208377\n",
      "\n",
      "Test set: Average loss: 0.4884, Accuracy: 503/624 (81%)\n",
      "\n",
      "Train Epoch: 10 [0/5248 (0%)]\tLoss: 0.126588\n",
      "Train Epoch: 10 [640/5248 (12%)]\tLoss: 0.142393\n",
      "Train Epoch: 10 [1280/5248 (24%)]\tLoss: 0.189063\n",
      "Train Epoch: 10 [1920/5248 (37%)]\tLoss: 0.288754\n",
      "Train Epoch: 10 [2560/5248 (49%)]\tLoss: 0.209815\n",
      "Train Epoch: 10 [3200/5248 (61%)]\tLoss: 0.248759\n",
      "Train Epoch: 10 [3840/5248 (73%)]\tLoss: 0.196128\n",
      "Train Epoch: 10 [4480/5248 (85%)]\tLoss: 0.170134\n",
      "Train Epoch: 10 [5120/5248 (98%)]\tLoss: 0.279470\n",
      "\n",
      "Test set: Average loss: 0.4349, Accuracy: 510/624 (82%)\n",
      "\n",
      "Train Epoch: 11 [0/5248 (0%)]\tLoss: 0.144592\n",
      "Train Epoch: 11 [640/5248 (12%)]\tLoss: 0.108393\n",
      "Train Epoch: 11 [1280/5248 (24%)]\tLoss: 0.154553\n",
      "Train Epoch: 11 [1920/5248 (37%)]\tLoss: 0.191510\n",
      "Train Epoch: 11 [2560/5248 (49%)]\tLoss: 0.188200\n",
      "Train Epoch: 11 [3200/5248 (61%)]\tLoss: 0.256786\n",
      "Train Epoch: 11 [3840/5248 (73%)]\tLoss: 0.150088\n",
      "Train Epoch: 11 [4480/5248 (85%)]\tLoss: 0.166692\n",
      "Train Epoch: 11 [5120/5248 (98%)]\tLoss: 0.110012\n",
      "\n",
      "Test set: Average loss: 0.4054, Accuracy: 515/624 (83%)\n",
      "\n",
      "Train Epoch: 12 [0/5248 (0%)]\tLoss: 0.251142\n",
      "Train Epoch: 12 [640/5248 (12%)]\tLoss: 0.164910\n",
      "Train Epoch: 12 [1280/5248 (24%)]\tLoss: 0.280972\n",
      "Train Epoch: 12 [1920/5248 (37%)]\tLoss: 0.248072\n",
      "Train Epoch: 12 [2560/5248 (49%)]\tLoss: 0.180743\n",
      "Train Epoch: 12 [3200/5248 (61%)]\tLoss: 0.109644\n",
      "Train Epoch: 12 [3840/5248 (73%)]\tLoss: 0.262026\n",
      "Train Epoch: 12 [4480/5248 (85%)]\tLoss: 0.242295\n",
      "Train Epoch: 12 [5120/5248 (98%)]\tLoss: 0.211670\n",
      "\n",
      "Test set: Average loss: 0.4994, Accuracy: 503/624 (81%)\n",
      "\n",
      "Train Epoch: 13 [0/5248 (0%)]\tLoss: 0.091566\n",
      "Train Epoch: 13 [640/5248 (12%)]\tLoss: 0.171802\n",
      "Train Epoch: 13 [1280/5248 (24%)]\tLoss: 0.139447\n",
      "Train Epoch: 13 [1920/5248 (37%)]\tLoss: 0.140855\n",
      "Train Epoch: 13 [2560/5248 (49%)]\tLoss: 0.239720\n",
      "Train Epoch: 13 [3200/5248 (61%)]\tLoss: 0.306802\n",
      "Train Epoch: 13 [3840/5248 (73%)]\tLoss: 0.114485\n",
      "Train Epoch: 13 [4480/5248 (85%)]\tLoss: 0.088345\n",
      "Train Epoch: 13 [5120/5248 (98%)]\tLoss: 0.320459\n",
      "\n",
      "Test set: Average loss: 0.4084, Accuracy: 518/624 (83%)\n",
      "\n",
      "Train Epoch: 14 [0/5248 (0%)]\tLoss: 0.149927\n",
      "Train Epoch: 14 [640/5248 (12%)]\tLoss: 0.113022\n",
      "Train Epoch: 14 [1280/5248 (24%)]\tLoss: 0.097709\n",
      "Train Epoch: 14 [1920/5248 (37%)]\tLoss: 0.143375\n",
      "Train Epoch: 14 [2560/5248 (49%)]\tLoss: 0.118564\n",
      "Train Epoch: 14 [3200/5248 (61%)]\tLoss: 0.210598\n",
      "Train Epoch: 14 [3840/5248 (73%)]\tLoss: 0.146185\n",
      "Train Epoch: 14 [4480/5248 (85%)]\tLoss: 0.161138\n",
      "Train Epoch: 14 [5120/5248 (98%)]\tLoss: 0.159387\n",
      "\n",
      "Test set: Average loss: 0.6047, Accuracy: 481/624 (77%)\n",
      "\n",
      "Train Epoch: 15 [0/5248 (0%)]\tLoss: 0.153065\n",
      "Train Epoch: 15 [640/5248 (12%)]\tLoss: 0.241365\n",
      "Train Epoch: 15 [1280/5248 (24%)]\tLoss: 0.120066\n",
      "Train Epoch: 15 [1920/5248 (37%)]\tLoss: 0.119085\n",
      "Train Epoch: 15 [2560/5248 (49%)]\tLoss: 0.202701\n",
      "Train Epoch: 15 [3200/5248 (61%)]\tLoss: 0.132675\n",
      "Train Epoch: 15 [3840/5248 (73%)]\tLoss: 0.111125\n",
      "Train Epoch: 15 [4480/5248 (85%)]\tLoss: 0.189916\n",
      "Train Epoch: 15 [5120/5248 (98%)]\tLoss: 0.117943\n",
      "\n",
      "Test set: Average loss: 0.6274, Accuracy: 477/624 (76%)\n",
      "\n",
      "Train Epoch: 16 [0/5248 (0%)]\tLoss: 0.136955\n",
      "Train Epoch: 16 [640/5248 (12%)]\tLoss: 0.175569\n",
      "Train Epoch: 16 [1280/5248 (24%)]\tLoss: 0.134207\n",
      "Train Epoch: 16 [1920/5248 (37%)]\tLoss: 0.184166\n",
      "Train Epoch: 16 [2560/5248 (49%)]\tLoss: 0.187424\n",
      "Train Epoch: 16 [3200/5248 (61%)]\tLoss: 0.116202\n",
      "Train Epoch: 16 [3840/5248 (73%)]\tLoss: 0.152591\n",
      "Train Epoch: 16 [4480/5248 (85%)]\tLoss: 0.127389\n",
      "Train Epoch: 16 [5120/5248 (98%)]\tLoss: 0.199285\n",
      "\n",
      "Test set: Average loss: 0.4144, Accuracy: 515/624 (83%)\n",
      "\n",
      "Train Epoch: 17 [0/5248 (0%)]\tLoss: 0.191413\n",
      "Train Epoch: 17 [640/5248 (12%)]\tLoss: 0.083315\n",
      "Train Epoch: 17 [1280/5248 (24%)]\tLoss: 0.082997\n",
      "Train Epoch: 17 [1920/5248 (37%)]\tLoss: 0.123823\n",
      "Train Epoch: 17 [2560/5248 (49%)]\tLoss: 0.219255\n",
      "Train Epoch: 17 [3200/5248 (61%)]\tLoss: 0.271109\n",
      "Train Epoch: 17 [3840/5248 (73%)]\tLoss: 0.197097\n",
      "Train Epoch: 17 [4480/5248 (85%)]\tLoss: 0.105214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [5120/5248 (98%)]\tLoss: 0.217323\n",
      "\n",
      "Test set: Average loss: 0.5015, Accuracy: 495/624 (79%)\n",
      "\n",
      "Train Epoch: 18 [0/5248 (0%)]\tLoss: 0.192665\n",
      "Train Epoch: 18 [640/5248 (12%)]\tLoss: 0.091325\n",
      "Train Epoch: 18 [1280/5248 (24%)]\tLoss: 0.150836\n",
      "Train Epoch: 18 [1920/5248 (37%)]\tLoss: 0.241556\n",
      "Train Epoch: 18 [2560/5248 (49%)]\tLoss: 0.226775\n",
      "Train Epoch: 18 [3200/5248 (61%)]\tLoss: 0.169813\n",
      "Train Epoch: 18 [3840/5248 (73%)]\tLoss: 0.091515\n",
      "Train Epoch: 18 [4480/5248 (85%)]\tLoss: 0.149328\n",
      "Train Epoch: 18 [5120/5248 (98%)]\tLoss: 0.131672\n",
      "\n",
      "Test set: Average loss: 0.4393, Accuracy: 519/624 (83%)\n",
      "\n",
      "Train Epoch: 19 [0/5248 (0%)]\tLoss: 0.128379\n",
      "Train Epoch: 19 [640/5248 (12%)]\tLoss: 0.177919\n",
      "Train Epoch: 19 [1280/5248 (24%)]\tLoss: 0.104773\n",
      "Train Epoch: 19 [1920/5248 (37%)]\tLoss: 0.161310\n",
      "Train Epoch: 19 [2560/5248 (49%)]\tLoss: 0.175549\n",
      "Train Epoch: 19 [3200/5248 (61%)]\tLoss: 0.234006\n",
      "Train Epoch: 19 [3840/5248 (73%)]\tLoss: 0.168069\n",
      "Train Epoch: 19 [4480/5248 (85%)]\tLoss: 0.101354\n",
      "Train Epoch: 19 [5120/5248 (98%)]\tLoss: 0.151634\n",
      "\n",
      "Test set: Average loss: 0.5228, Accuracy: 494/624 (79%)\n",
      "\n",
      "Train Epoch: 20 [0/5248 (0%)]\tLoss: 0.140852\n",
      "Train Epoch: 20 [640/5248 (12%)]\tLoss: 0.255205\n",
      "Train Epoch: 20 [1280/5248 (24%)]\tLoss: 0.179029\n",
      "Train Epoch: 20 [1920/5248 (37%)]\tLoss: 0.200537\n",
      "Train Epoch: 20 [2560/5248 (49%)]\tLoss: 0.058587\n",
      "Train Epoch: 20 [3200/5248 (61%)]\tLoss: 0.163811\n",
      "Train Epoch: 20 [3840/5248 (73%)]\tLoss: 0.174837\n",
      "Train Epoch: 20 [4480/5248 (85%)]\tLoss: 0.203312\n",
      "Train Epoch: 20 [5120/5248 (98%)]\tLoss: 0.109210\n",
      "\n",
      "Test set: Average loss: 0.7032, Accuracy: 475/624 (76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
